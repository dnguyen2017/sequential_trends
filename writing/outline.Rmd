---
title: "Sequential tests for rapid detection of population decline"
author: "David Nguyen"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
my_theme <- function(...) theme_minimal(base_size = 15) 

# Created by Easton White
# Created on 17-Jan-2017
# Last edited on 18-Jan-2017

# this is a function to create a list of all possible sub samples of a time series. For example, a time series of length ten could have 9 possible 2 year continuous subsamples, 8 possible, 3 year continuous subsamples, and so forth until 1 possible 10 year subsample
create_subsamples = function(pop){
  subsamples=list()
  for (n in 1:(length(pop)-1)){
    subsamples[[n]] = ldply(1:length(pop), function(x){pop[x:(x+n)]})
  }
  return(subsamples)
}  
```

# Intro
Conservationists need to know two things: 

1. What is the population trend? 
2. what is the "best" intervention to assist or reverse this trend?

* Describe common methods used in conservation management.
* Note, that the general problems (1 & 2) are encountered in clinical research
* describe benefits of sequential methods in clinical research and why these benefits are also attractive for conservation management:
    + fewer samples required (on average)
    + allows for flexibility (e.g., fully sequential and interim analyses)
    + allows for early stopping of trial based on "futility"
    
```{r}
# linear population trend model w/ normal process error
sim_lin_proc <- function (x0, t, trend, sd) {
  # allow for constant trend or vector of varying trends (automatically recycles vector if lenght(trend) < t)
  trend_val = rep(trend, length = t)
  sd_val = rep(sd, length = t)
  
  pop <- vector("numeric", length = t)
  pop[1] <- x0
  for (i in 1:(length(pop) - 1)) {
    pop[i + 1] <- rnorm(1, pop[i] + trend_val[i], sd_val[i])
  }
  return(tibble(time = 1:t, pop = pop, trend = trend_val, sd = sd_val))
}

obs_model <- function (x, sd) {
  # normal and unbiased observation noise
  obs <- x + rnorm(n = length(x), mean = 0, sd = sd)
  return(obs)
}

multi_sim <- function(nsim, x0, t, trend, sd) {
  simulations <- lapply(1:nsim, function(x) sim_lin_proc(x0, t, trend, sd))
 return(bind_rows(simulations, .id = "simulation") %>% mutate(simulation = as.numeric(simulation)))
}

```

```{r}
# simulate model
set.seed(123)
sim_df <- multi_sim(100, 10^3, 100, 1.5, 5)
# sim_list <- lapply(1:100, function(x) sim_pop(10^3, 100, 1.5, 5))
# sim_df <- bind_rows(sim_list, .id = "simulation")
```

```{r}
get_pval <- function(model) {
  return(summary(model)$coefficients[2,4])
}
get_slope <- function(model) {
  return(model$coefficients[2])
}

calc_lm <- function (data, nsim) {
  # pre-allocate a list to store dfs
  lm_list <- vector("list", length = nsim)
  
  # fit lm and extract slope estimates and p-values
  for (i in seq_along(lm_list)) {
    # create empty cols for pval and trend estimate
    data$pval <- NA_real_
    data$trend_est <- NA_real_
    
    # grab current time series
    current_ts <- data %>% filter(simulation == i)
    
    # fit lm to all subsets t \in {{1,2}, {1,2,3}, {1,2,3,4} ...} in current_ts and extract slope est and pval
    for (j in 2:nrow(current_ts)) {
      lm_fit <- lm(pop ~ time, data = current_ts[1:j,])
      current_ts[j, "trend_est"] <- get_slope(lm_fit)
      current_ts[j, "pval"] <- get_pval(lm_fit)
    }
    # save calculated values
    lm_list[[i]] <- current_ts
    
  }
  
  return(bind_rows(lm_list))
}

get_conclusion <- function(data, sig_level) {
  data %>% 
    mutate(significant = ifelse(pval <= sig_level, TRUE, FALSE),
         correct_sign = ifelse(sign(trend_est) == sign(trend), TRUE, FALSE),
         correct_inference = ifelse(significant == TRUE & correct_sign == TRUE, TRUE, FALSE),
         type_error = case_when(correct_sign == FALSE & significant == FALSE ~ "insignificant_false",
                                correct_sign == FALSE & significant == TRUE ~ "significant_false",
                                correct_sign == TRUE & significant == FALSE ~ "insignificant_true",
                                correct_sign == TRUE & significant == TRUE ~ "significant_true")) %>%
    return()
}

suff_power <- function(x) x >= 80

get_power <- function(data, power_level = 0.8, nsim) {
  data %>%
    group_by(time) %>%
    count(type_error) %>%
    filter(type_error == "significant_true") %>%
    ungroup() %>%
    mutate(time_power = ifelse(n >= nsim * power_level, time, NA),
           mintime = min(time_power, na.rm = TRUE)) %>%
    select(-time_power) %>%
    return()
}
```

```{r eval = FALSE}
lm_conclusions <- calc_lm(sim_df, nsim = 100) %>% get_conclusion(sig_level = 0.05)
lm_power <- lm_conclusions %>% get_power(nsim = 100)

# lm_df %>%
#   group_by(time) %>%
#   count(type_error) %>%
#   filter(type_error == "significant_true") %>%
#   ungroup() %>%
#   mutate(time_power = ifelse(n >= 80, time, NA),
#          mintime = min(time_power, na.rm = TRUE)) %>%
#   filter(time > 2) %>%
lm_power %>%
  ggplot() +
  geom_line(aes(x = time, y = n/100, col = type_error)) +
  geom_hline(aes(yintercept = 80/100), linetype = "dashed") +
  geom_vline(aes(xintercept = mintime), linetype = "dashed")



# find minimum time to get a slope estimate that is significant (p <= 0.05) and the correct sign
lm_conclusions %>%
#lm_df %>%
  group_by(simulation) %>%
  summarize(min_time = detect_index(correct_inference, isTRUE)) %>%
  ggplot() +
  geom_histogram(aes(x = min_time), binwidth = 1)

```


```{r}
# one sided sprt calculation
objfn <- function(trend, x1, x0, sd) {
  # likelihood( x1 | x0, r, dispersion )
  dnorm(x = x1, mean = x0 + trend, sd = sd)
}


#  find restricted MLE under h_0 or h_1
likelihood <- function (x_prev, x_now, sd, interval = c(), guess = 1.5) {
  opt <- optim(par = guess, fn = objfn, control = list(fnscale = -1), 
      method = "Brent", lower = interval[1], upper = interval[2],
      x0 = x_prev, x1 = x_now, sd = sd)
  return(opt$value)
}

# sequential test functions
calc_sr <- function(data, accept_reg = c(0,0), reject_reg = c(0, 5)) {
  
  # initialize empty columns for likelihood calculations
  data$lik0 <- NA_real_
  data$lik1 <- NA_real_
  
  # parameter space
  theta0 <- accept_reg
  theta1 <- reject_reg
  
  # use midpoint of interval as init guess for optim (Brent's method)
  guess0 <- accept_reg[1] + (accept_reg[2] - accept_reg[1]) / 2 
  guess1 <- reject_reg[1] + (reject_reg[2] - reject_reg[1]) / 2 
  
  sim_number <- 1
  for (i in 2:nrow(data)) {
    
    if (data[i, "simulation"] == sim_number + 1) {
      # skip calculation of likelihood for the first observation in each simluation
      sim_number <- sim_number + 1
      next
    }
    
    # observed states and known pars
    x0 <- unlist(data[i-1, "pop"])
    x1 <- unlist(data[i, "pop"])
    sd <- unlist(data[i, "sd"])
    
    # likelihood calculation
    data[i,"lik0"] <- dnorm(x = x1, mean = x0, sd = sd) # H0: x_t+1 ~ N(x_t, sd)
    data[i,"lik1"] <- likelihood(x_prev = x0, x_now = x1, sd = sd,  # p(x_now | x_prev, disp, theta \in Theta_1)
                                   interval = theta1, guess = guess1)
  }
  
  return(data)
}
```

```{r time_to_detection}
# calculate time required to detect trend
sprt_df <- calc_sr(sim_df)

sprt_df <- 
  sprt_df %>%  
  group_by(simulation) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.2)/ 0.05),
         b = log(0.2/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))

is_h1 <- function(x) x == "H1"

delay_df <- 
  sprt_df %>% ungroup() %>%
  group_by(simulation) %>%
  summarize(delay = unique(detect_index(decision, is_h1))) %>%
  mutate(mean_delay = mean(delay),
         median_delay = median(delay),
         p20_delay = quantile(delay, probs = 0.2),
         p80_delay = quantile(delay, probs = 0.8),
         p90_delay = quantile(delay, probs = 0.9))
```

```{r sensitivity_setup, cache = TRUE}
# sensitivity analysis following Fig. 1 c-d in White 2019
set.seed(123)
trend_var <- -seq(1, 3, by = 0.2)
sd_var <- seq(1, 6, by = 0.5)
trend_cst <- -rep(1.5, length = length(sd_var))
sd_cst <- rep(5, length = length(trend_var))

trends <- c(trend_var, trend_cst)
sds    <- c(sd_cst, sd_var)

n_sims <- 50
n_steps <- 50
sensitivity_list <- lapply(seq_along(trends), 
                           function(x) multi_sim(n_sims, 10^3, n_steps, trends[x], sds[x]))
sens_df <- bind_rows(sensitivity_list)
```

```{r plot_sens_time_series, eval = FALSE, echo = FALSE}
sens_df %>%
  ggplot() +
  geom_line(aes(x = time, y = pop, group = simulation)) +
  facet_wrap(~interaction(sd, trend) )
```

```{r sens_sprt, cache = TRUE, dependson="sensitivity_setup"}
# sprt calculations
sens_sprt <- sens_df %>% group_by(sd, trend) %>% calc_sr(reject_reg = c(-5,0))

# alpha = 0.05, beta = 0.2 (following White 2019)
sens_sprt <- 
  sens_sprt %>%  
  group_by(simulation, trend, sd) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.2)/ 0.05),
         b = log(0.2/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))
```

```{r summarize_sens_delay, echo = FALSE}
sens_delay <- 
  sens_sprt %>% ungroup() %>%
  group_by(simulation, trend, sd) %>%
  summarize(delay = unique(detect_index(decision, is_h1))) %>%
  ungroup() %>% group_by(trend, sd) %>%
  mutate(mean_delay = mean(delay),
         median_delay = median(delay),
         p20_delay = quantile(delay, probs = 0.2),
         p80_delay = quantile(delay, probs = 0.8),
         p90_delay = quantile(delay, probs = 0.9),
         max_delay = max(delay))
```

# Trend detection: sequential vs fixed sample approaches
Managers need to detect population trends in a timely and reliable manner. Timeliness is reflected in the number of samples (e.g., years) needed to detect a trend. Reliability is captured by the type I error and power of a test.

Linear regression is commonly used to detect trends in population data. For instance,
$$ N_{t+1} \sim N(N_t + r,\sigma), $$
Where $N_t$ is the population size at time $t$, $r$ is the population trend, and $\sigma$ is the population variability.

To identify the timeliness and reliability of linear regression for detecting population trends, [White (2019)](https://academic.oup.com/bioscience/article/69/1/40/5195956#129750432) repeatedly fit linear models to sub-samples of a time series to identify the minimum sample size needed to achieve a specified type I error and power. That is, for a time series with $T$ time points, he fit a linear regression model to each contiguous sub-samples of size $2, 3, \ldots, T-1$. Using the arbitrary (but conventional) benchmark of 0.8 power at the 0.05 significance level, White used the regression outputs to find the minimum sub-sample length such that 80 % of the samples had a significant regression slope coefficient. This is essentially a sample size calculation for a one-sided hypothesis test $H_0: r = 0$ vs $H_1: r > 0$.

We can also test this one-sided hypothesis using a sequential test. We can define our sequential test statistic $(S)$ as:

\begin{align*}
S_0 & = 0 \\
S_n & = \frac{P(N_{t+1} | N_t, \hat{r}, \sigma)}{P(N_{t+1} | N_t, r = 0, \sigma)}
\end{align*}

where $\hat{r}$ is the MLE of r constrained to  $r \in (0,\inf)$ or $r \in (-\inf, 0)$ depending on whether detection of a linear increase or decrease is desired.

The decision rule is:

\begin{align*}
    \begin{cases}
      \text{reject } H_0  & \Sigma^{n}_{r = 0} S_r > \log(A) \\
      \text{accept } H_0 & \Sigma^{n}_{r = 0} S_r \leq \log(B) \\
      \text{continue sampling} & \log(B)  <  \Sigma^{n}_{r = 0} S_r  \leq  \log(A)  
    \end{cases}
\end{align*}

The thresholds are set following Wald's method such that $A \sim \frac{1-\beta}{\alpha}$ and $B \sim \frac{\beta}{1-\alpha}$ where $\alpha$ and $\beta$ is the probabilities of type I and II error, respectively.

Here, I do a simulation study to assess the performance of a one-sided sequential generalized likelihood ratio test (SGLRT) that is directly comparable to the power analysis shown in Figure 1 of White 2019.

## Minimum sample size to detect population trend (from White 2019)

<!-- ![](white_2019_fig_1.jpeg) -->

```{r}
calc_sens_lm <- function (data, sds, trends) {
  nsim <- data %>% pull(simulation) %>% as.numeric() %>% max()
  tfinal <- data %>% pull(time) %>% max() 
  # sd_vals <- data %>% pull(sd) %>% unique()
  # trend_vals <- data %>% pull(trend) %>% unique()
  
  full_list <- vector("list", length = length(sds)) # each entry is a unique parameter combination
  temp_list <- vector("list", length = nsim)       # each entry is a simulation number for a particular parameter combination
  
  data$trend_est <- NA_real_; data$pval <- NA_real_
  for ( i in 1:length(sds)) { # for each parameter combination
    sd_now <- sds[i]
    trend_now <- trends[i]
    
    for (j in seq_along(temp_list)) { # for each simulatin of a parameter combination
      current_ts <- data %>% filter(simulation == j,
                                    near(sd, sd_now), 
                                    near(trend, trend_now))
      #print(current_ts)
      #current_ts$trend_est <- NA
      #current_ts$pval <- NA
      
      for (k in 2:tfinal) { # for each subset {{1,2}, {1,2,3}, {1,2,3,4,}, ...}
        current_data <- current_ts[1:k,]
        #if(sum(is.na(current_data$pop)) > 0 |  sum(is.na(current_data$time)) > 0) print(c(sds[i],trends[i]))
        current_lm <- lm(pop ~ time, data = current_ts[1:k,]) 
        #return(current_ts[k, "trend_est"])
        #if(is.na(get_slope(current_lm))) print(c(i,j,k))
        current_ts[k, "trend_est"] <- get_slope(current_lm) %>% unname()
        current_ts[k, "pval"] <- get_pval(current_lm) %>% unname()
      }
      #print(c(sd_now,trend_now))
      temp_list[[j]] <- current_ts # save result for a particular simulation
    }
    full_list[[i]] <- bind_rows(temp_list) # save result for a particular parameter combination
  }
  return(bind_rows(full_list))
}

#sens_df %>% calc_sens_lm(sds = sds, trends = trends)
#sens_df %>% filter(is.na(time))

# sens_df %>% filter(near(simulation,1), near(sd, 1), near(trend,1))
# sens_df %>% pull(sd) %>% unique() %>% str()
# sds
```


```{r calc_sens_lm, cache = TRUE, dependson="sensitivity_setup"}
sens_lm <- 
  sens_df %>% #filter(time %in% 1:50) %>%
  #filter(simulation %in% 1:10) %>%
  #group_by(trend, sd) %>%
  calc_sens_lm(sds = sds, trends = trends) %>%
  group_by(trend, sd) %>%
  get_conclusion(sig_level = 0.05) %>%
    group_by(time, trend, sd) %>% 
  mutate(correct = sum(correct_inference, na.rm = TRUE), 
         power = correct/max(simulation)) %>%
  mutate(sufficient_power = ifelse(power >= 0.8, time, NA)) %>%
  group_by(trend, sd) %>%
  mutate(mintime = min(sufficient_power, na.rm = TRUE))
# 
# sens_conclusion <- 
#   sens_lm %>% 
#   group_by(trend, sd) %>%
#   get_conclusion(sig_level = 0.05)
# 
# 
# sens_power <-
#   sens_conclusion %>% 
#   group_by(time, trend, sd) %>% 
#   mutate(correct = sum(correct_inference, na.rm = TRUE), 
#          power = correct/10) %>%
#   mutate(sufficient_power = ifelse(correct >= 8, time, NA)) %>%
#   group_by(trend, sd) %>%
#   mutate(mintime = min(sufficient_power, na.rm = TRUE))


```


```{r out.width="50%", fig.show="hold"}
# sens_lm %>%
#   filter(sd == 5) %>%
  ggplot() +
  geom_line(data = filter(sens_lm, sd ==5), aes(x = trend, y = mintime), col = "red") +
  geom_line(data = filter(sens_delay, sd ==5), aes(x = trend, y =mean_delay)) +
   lims(y = c(0, 50)) +
  labs(title = "c. Effect of trend strength on detection time",
       x = "Trend strength (slope coefficient)", y = "Time to detection") +
  my_theme()

# sens_lm %>%
#   filter(trend == 1.5) %>%
  ggplot() +
  geom_line(data = filter(sens_lm, trend ==unique(trend_cst) ), aes(x = sd, y = mintime), col = "red") +
  geom_line(data = filter(sens_delay, trend ==unique(trend_cst)), aes(x = sd, y = mean_delay)) +
  lims(y = c(0, 50)) +
  labs(title = "d. Effect of population variability on detection time",
       x = "Population variability (sd)", y = "Time to detection") +
  my_theme()
```


## Sample sizes needed to detect trend using SGLRT
```{r plot_example, out.width = "50%", fig.show = "hold", echo = FALSE}
# plot example simulations
sim_df %>% ggplot() + 
  geom_line(aes(x = time, y = pop, group = simulation), alpha = 0.5) + 
  labs(title = "a. Simulated examples of linear population trend",
       y = "Population size",
       x = "Time") +
  my_theme()

# plot distribution of time that trend was detected
ggplot(delay_df) + 
  geom_bar(aes(x = delay)) +
  labs(title = "b. Distribution of trend detection times",
       y = "Frequency",
       x = "Time") +
  geom_vline(aes(xintercept = mean_delay), size = 1.5) +
  geom_vline(aes(xintercept = p80_delay), linetype = "dashed", size = 1.5) +
  geom_vline(aes(xintercept = p90_delay), linetype = "dotdash", size = 1.5) +
  my_theme()
  
```


```{r plot_sens_delay, out.width="50%", fig.show = "hold", echo = FALSE}
line_width  <- 1.5
sens_delay %>%
  filter(sd == unique(sd_cst)) %>%
  ggplot() +
  geom_line(aes(x = trend, y = mean_delay), linetype = "solid", size = line_width) +
  geom_line(aes(x = trend, y = p80_delay), linetype = "dashed", size = line_width) +
  geom_line(aes(x = trend, y = p90_delay), linetype = "dotdash", size = line_width) +
  geom_line(aes(x = trend, y = max_delay), linetype = "dotted", size = line_width) +
  geom_line(data = filter(sens_lm, sd == unique(sd_cst)), aes(x = trend, y = mintime), col = "red") +
  lims(y = c(0, 50)) +
  labs(title = "c. Effect of trend strength on detection time",
       x = "Trend strength (slope coefficient)", y = "Time to detection") +
  my_theme()
  

sens_delay %>%
  filter(trend == unique(trend_cst)) %>%
  ggplot() +
  geom_line(aes(x = sd, y = mean_delay), linetype = "solid", size = line_width) +
  geom_line(aes(x = sd, y = p80_delay), linetype = "dashed", size = line_width) +
  geom_line(aes(x = sd, y = p90_delay), linetype = "dotdash", size = line_width) +
  geom_line(aes(x = sd, y = max_delay), linetype = "dotted", size = line_width) +
  geom_line(data = filter(sens_lm, trend == unique(trend_cst)), aes(x = sd, y = mintime), col = "red") +
  lims(y = c(0, 50)) +
  labs(title = "d. Effect of population variability on detection time",
       x = "Population variability (sd)", y = "Time to detection") +
  my_theme()

```

For panels c and d: 

* solid = mean detection time
* dashed = 80^th^ percentile of detection time
* dot-dash = 90^th^ percentile of detection time
* dotted = maximum detection time
    
## Initial impressions

On average, the SGLRT detects trends faster than the fixed-sample size test. While the maximum sample size under SGLRT can be similar or much larger than the fixed-sample size, this appears to be rare judging by the 90^th^ percentile detection times.

<!-- 1. Effect of trend strength on detection -->
<!--     + On average, the sequential test requires fewer samples than the fixed-sample procedure (Fig. 1 c.solid line). This is also true of the 80^th^ percentile run length for smaller trend strengths, the run length is similar to the fixed-sample approach when the trend is strong. -->
<!--     + Interestingly, the longest run lengths of the sequential test appear to be similiar to the minimum samples needed under the fixed-sample approach. (With the caveat that more extreme values of the maximum run length are likely to appear for larger numbers of simulations. For this simple model, the maximum run length may have been calculated analytically [at least for simple hypothesis situation]) -->

<!-- 2. Effect of population variability on detection -->
<!--     + The average run length was comparable or less than the minimum sample size needed for the fixed-sample approach. They appear to be the same for lower variability, but the number of samples needed as variability increases grows more slowly for the sequential approach than the fixed-sample approach. -->
<!--     + The maximum run length appears to grow more slowly than the minimum sample size needed under the fixed sample approach. -->
<!--     + note, that the current sequential test I'm using includes the true population variability as a parameter. So, the test would likely perform worse (more similar to  fixed sample procedure) if variability was unknown and needed to be estimated. -->
    
## Takeaways
Sequential testing approaches on average out-perform a common method that is used to detect population trends and for power analyses to determine sample sizes for monitoring design. If the promise of these preliminary results holds up, introducing sequential approaches to population monitoring and trend detection would have an important impact on research practices for conservation management,

# extensions

## immediate

* Detecting time-varying trends. A problem shared by the current sequential test (and the fixed sample approach) is that it doesn't deal well with time-varying trends. In both simulation studies, it is assumed that the trend is constant. Sequential methods allowing for changing trends (sliding window or weighted tests) will be an important generalization and provide another improvement over the basic linear regression approach.

* Estimation of population variability. Need to check how sequential test performs when variability is unknown and an estimate must be used the the calculation of the test statistic.

## longer term

* spatial replication: how can we leverage spatial replicates to improve detection of trends? A fixed sample simulation study of this problem is [Rhodes and Jonzen (2011)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2011.06370.x). This is a common case in sequential analysis literature.

* sequential methods for evaluating management alternatives: Here is where I would need to leverage the connections between clinical trials and wildlife management. May be a fruitful extension. Need to understand how it dovetails with adaptive management approaches (dynamic programming is common to both approaches).