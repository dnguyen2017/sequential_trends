---
title: "Sequential tests for rapid detection of population decline"
author: "David Nguyen"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
my_theme <- function(...) theme_minimal(base_size = 15) 
```

# Intro
Conservationists need to know two things: 

1. What is the population trend? 
2. what is the "best" intervention to assist or reverse this trend?

* Describe common methods used in conservation management.
* Note, that the general problems (1 & 2) are encountered in clinical research
* describe benefits of sequential methods in clinical research and why these benefits are also attractive for conservation management:
    + fewer samples required (on average)
    + allows for flexibility (e.g., fully sequential and interim analyses)
    + allows for early stopping of trial based on "futility"
    
```{r}
# linear population trend model w/ normal error
sim_pop <- function (x0, t, trend, sd) {
  pop <- vector("numeric", length = t)
  pop[1] <- x0
  for (i in 1:(length(pop) - 1)) {
    pop[i + 1] <- rnorm(1, pop[i] + trend, sd)
  }
  return(tibble(time = 1:t, pop = pop, trend = trend, sd = sd))
}

multi_sim <- function(nsim, x0, t, trend, sd) {
  simulations <- lapply(1:nsim, function(x) sim_pop(x0, t, trend, sd))
  return(bind_rows(simulations, .id = "simulation"))
}

```

```{r}
# simulate model
set.seed(123)
sim_df <- multi_sim(100, 10^3, 100, 1.5, 5)
# sim_list <- lapply(1:100, function(x) sim_pop(10^3, 100, 1.5, 5))
# sim_df <- bind_rows(sim_list, .id = "simulation")
```

```{r}
# one sided sprt calculation
objfn <- function(trend, x1, x0, sd) {
  # likelihood( x1 | x0, r, dispersion )
  dnorm(x = x1, mean = x0 + trend, sd = sd)
}


#  find restricted MLE under h_0 or h_1
likelihood <- function (x_prev, x_now, sd, interval = c(), guess = 1.5) {
  opt <- optim(par = guess, fn = objfn, control = list(fnscale = -1), 
      method = "Brent", lower = interval[1], upper = interval[2],
      x0 = x_prev, x1 = x_now, sd = sd)
  return(opt$value)
}

# sequential test functions
calc_sr <- function(data, accept_reg = c(0,0), reject_reg = c(0, 5)) {
  
  # initialize empty columns for likelihood calculations
  data$lik0 <- NA
  data$lik1 <- NA
  
  # parameter space
  theta0 <- accept_reg
  theta1 <- reject_reg
  
  # use midpoint of interval as init guess for optim (Brent's method)
  guess0 <- accept_reg[1] + (accept_reg[2] - accept_reg[1]) / 2 
  guess1 <- reject_reg[1] + (reject_reg[2] - reject_reg[1]) / 2 
  
  sim_number <- 1
  for (i in 2:nrow(data)) {
    
    if (data[i, "simulation"] == sim_number + 1) {
      # skip calculation of likelihood for the first observation in each simluation
      sim_number <- sim_number + 1
      next
    }
    
    # observed states and known pars
    x0 <- unlist(data[i-1, "pop"])
    x1 <- unlist(data[i, "pop"])
    sd <- unlist(data[i, "sd"])
    
    # likelihood calculation
    data[i,"lik0"] <- dnorm(x = x1, mean = x0, sd = sd) # H0: x_t+1 ~ N(x_t, sd)
    data[i,"lik1"] <- likelihood(x_prev = x0, x_now = x1, sd = sd,  # p(x_now | x_prev, disp, theta \in Theta_1)
                                   interval = theta1, guess = guess1)
  }
  
  return(data)
}
```

```{r time_to_detection}
# calculate time required to detect trend
sprt_df <- calc_sr(sim_df)

sprt_df <- 
  sprt_df %>%  
  group_by(simulation) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.2)/ 0.05),
         b = log(0.2/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))

is_h1 <- function(x) x == "H1"

delay_df <- 
  sprt_df %>% ungroup() %>%
  group_by(simulation) %>%
  summarize(delay = unique(detect_index(decision, is_h1))) %>%
  mutate(mean_delay = mean(delay),
         median_delay = median(delay),
         p20_delay = quantile(delay, probs = 0.2),
         p80_delay = quantile(delay, probs = 0.8))
```

```{r sensitivity_setup, cache = TRUE}
# sensitivity analysis following Fig. 1 c-d in White 2019
set.seed(123)
trend_var <- seq(1, 3, by = 0.5)
sd_var <- 1:6
trend_cst <- rep(1.5, length = length(sd_var))
sd_cst <- rep(5, length = length(trend_var))

trends <- c(trend_var, trend_cst)
sds    <- c(sd_cst, sd_var)

sensitivity_list <- lapply(seq_along(trends), 
                           function(x) multi_sim(100, 10^3, 100, trends[x], sds[x]))
sens_df <- bind_rows(sensitivity_list)
```

```{r plot_sens_time_series, eval = FALSE, echo = FALSE}
sens_df %>%
  ggplot() +
  geom_line(aes(x = time, y = pop, group = simulation)) +
  facet_wrap(~interaction(sd, trend) )
```

```{r sens_sprt, cache = TRUE, dependson="sensitivity_setup"}
# sprt calculations
sens_sprt <- sens_df %>% group_by(sd, trend) %>% calc_sr()

# alpha = 0.05, beta = 0.2 (following White 2019)
sens_sprt <- 
  sens_sprt %>%  
  group_by(simulation, trend, sd) %>%
  mutate(sr = log(lik1/lik0) ,
         sr = ifelse(is.na(sr), 0, sr),
         sprt = cumsum(sr),
         a = log( (1 - 0.2)/ 0.05),
         b = log(0.2/(1 - 0.05)),
         decision = case_when(sprt <= a & sprt >= b ~"?",
                              sprt > a ~ "H1",
                              sprt < b ~ "H0"))
```

```{r summarize_sens_delay, echo = FALSE}
sens_delay <- 
  sens_sprt %>% ungroup() %>%
  group_by(simulation, trend, sd) %>%
  summarize(delay = unique(detect_index(decision, is_h1))) %>%
  ungroup() %>% group_by(trend, sd) %>%
  mutate(mean_delay = mean(delay),
         median_delay = median(delay),
         p20_delay = quantile(delay, probs = 0.2),
         p80_delay = quantile(delay, probs = 0.8),
         max_delay = max(delay))
```

# Trend detection: sequential vs fixed sample approaches
Managers need to detect population trends in a timely and reliable manner. Timeliness is reflected in the number of samples (e.g., years) needed to detect a trend. Reliability is captured by the type I error and power of a test.

Linear regression is commonly used to detect trends in population data. For instance,
$$ N_{t+1} \sim N(N_t + r,\sigma), $$
Where $N_t$ is the population size at time $t$, $r$ is the population trend, and $\sigma$ is the population variability.

To identify the timeliness and reliability of linear regression for detecting population trends, [White (2019)](https://academic.oup.com/bioscience/article/69/1/40/5195956#129750432) repeatedly fit linear models to sub-samples of a time series to identify the minimum sample size needed to achieve a specified type I error and power. That is, for a time series with $T$ time points, he fit a linear regression model to each contiguous sub-samples of size $2, 3, \ldots, T-1$. Using the arbitrary (but conventional) benchmark of 0.8 power at the 0.05 significance level, White used the regression outputs to find the minimum sub-sample length such that 80 % of the samples had a significant regression slope coefficient. This is essentially a sample size calculation for a one-sided hypothesis test $H_0: r = 0$ vs $H_1: r > 0$.

We can also test this one-sided hypothesis using a sequential test. We can define our sequential test statistic $(S)$ as:

\begin{align*}
S_0 & = 0 \\
S_n & = \frac{P(N_{t+1} | N_t, \hat{r}, \sigma)}{P(N_{t+1} | N_t, r = 0, \sigma)}
\end{align*}

where $\hat{r}$ is the MLE of r constrained to  $r \in (0,\inf)$ or $r \in (-\inf, 0)$ depending on whether detection of a linear increase or decrease is desired.

The decision rule is:

\begin{align*}
    \begin{cases}
      \text{reject } H_0  & \Sigma^{n}_{r = 0} S_r > \log(A) \\
      \text{accept } H_0 & \Sigma^{n}_{r = 0} S_r \leq \log(B) \\
      \text{continue sampling} & \log(B)  <  \Sigma^{n}_{r = 0} S_r  \leq  \log(A)  
    \end{cases}
\end{align*}

The thresholds are set following Wald's method such that $A \sim \frac{1-\beta}{\alpha}$ and $B \sim \frac{\beta}{1-\alpha}$ where $\alpha$ and $\beta$ is the probabilities of type I and II error, respectively.

Here, I do a simulation study to assess the performance of a one-sided sequential generalized likelihood ratio test (SGLRT) that is directly comparable to the power analysis shown in Figure 1 of White 2019.

## Minimum sample size to detect population trend (from White 2019)
![](white_2019_fig_1.jpeg)

## Sample sizes needed to detect trend using SGLRT
```{r plot_example, out.width = "50%", fig.show = "hold", echo = FALSE}
# plot example simulations
sim_df %>% ggplot() + 
  geom_line(aes(x = time, y = pop, group = simulation), alpha = 0.5) + 
  labs(title = "a. Simulated examples of linear population trend",
       y = "Population size",
       x = "Time") +
  my_theme()

# plot distribution of time that trend was detected
ggplot(delay_df) + 
  geom_bar(aes(x = delay)) +
  labs(title = "b. Distribution of trend detection times",
       y = "Frequency",
       x = "Time") +
  my_theme()
  #geom_vline(aes(xintercept = mean_delay)) +
  #geom_vline(aes(xintercept = median_delay), linetype = "dashed")
```


```{r plot_sens_delay, out.width="50%", fig.show = "hold", echo = FALSE}
line_width  <- 1.5
sens_delay %>%
  filter(sd == 5) %>%
  ggplot() +
  geom_line(aes(x = trend, y = mean_delay), linetype = "solid", size = line_width) +
  geom_line(aes(x = trend, y = p80_delay), linetype = "dashed", size = line_width) +
  geom_line(aes(x = trend, y = max_delay), linetype = "dotted", size = line_width) +
  lims(y = c(0, 50)) +
  labs(title = "c. Effect of trend strength on detection time",
       x = "Trend strength (slope coefficient)", y = "Time to detection") +
  my_theme()
  

sens_delay %>%
  filter(trend == 1.5) %>%
  ggplot() +
  geom_line(aes(x = sd, y = mean_delay), linetype = "solid", size = line_width) +
  geom_line(aes(x = sd, y = p80_delay), linetype = "dashed", size = line_width) +
  geom_line(aes(x = sd, y = max_delay), linetype = "dotted", size = line_width) +
  lims(y = c(0, 50)) +
  labs(title = "d. Effect of population variability on detection time",
       x = "Population variability (sd)", y = "Time to detection") +
  my_theme()
```

For panels c and d: 

* solid = mean detection time
* dashed = 80^th^ percentile of detection time
* dotted = maximum detection time
    
## Initial impressions

1. Effect of trend strength on detection
    + On average, the sequential test requires fewer samples than the fixed-sample procedure (Fig. 1 c.solid line). This is also true of the 80^th^ percentile run length for smaller trend strengths, the run length is similar to the fixed-sample approach when the trend is strong.
    + Interestingly, the longest run lengths of the sequential test appear to be similiar to the minimum samples needed under the fixed-sample approach. (With the caveat that more extreme values of the maximum run length are likely to appear for larger numbers of simulations. For this simple model, the maximum run length may have been calculated analytically [at least for simple hypothesis situation])
    
2. Effect of population variability on detection
    + The average run length was comparable or less than the minimum sample size needed for the fixed-sample approach. They appear to be the same for lower variability, but the number of samples needed as variability increases grows more slowly for the sequential approach than the fixed-sample approach.
    + The maximum run length appears to grow more slowly than the minimum sample size needed under the fixed sample approach.
    + note, that the current sequential test I'm using includes the true population variability as a parameter. So, the test would likely perform worse (more similar to  fixed sample procedure) if variability was unknown and needed to be estimated.
    
## Takeaways
Sequential testing approaches may out-perform a common method that is used to detect population trends and for power analyses to determine sample sizes for monitoring design. If the promise of these preliminary results holds up, introducing sequential approaches to population monitoring and trend detection would have an important impact on research practices for conservation management,

# extensions

## immediate

* Detecting time-varying trends. A problem shared by the current sequential test (and the fixed sample approach) is that it doesn't deal well with time-varying trends. In both simulation studies, it is assumed that the trend is constant. Sequential methods allowing for changing trends (sliding window or weighted tests) will be an important generalization and provide another improvement over the basic linear regression approach.

* Estimation of population variability. Need to check how sequential test performs when variability is unknown and an estimate must be used the the calculation of the test statistic.

## longer term

* spatial replication: how can we leverage spatial replicates to improve detection of trends? A fixed sample simulation study of this problem is [Rhodes and Jonzen (2011)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2011.06370.x). This is a common case in sequential analysis literature.

* sequential methods for evaluating management alternatives: Here is where I would need to leverage the connections between clinical trials and wildlife management. May be a fruitful extension. Need to understand how it dovetails with adaptive management approaches (dynamic programming is common to both approaches).