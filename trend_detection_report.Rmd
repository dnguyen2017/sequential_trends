---
title: "ad hoc methods for detecting population trends"
author: "David Nguyen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(tidyverse)
source("trend_detection_functions.R")
```


```{r simulation, cache = T, results = FALSE}
knitr::read_chunk("trend_detection_functions.R")
set.seed(123)
numsim <- 100
trend_ <- 1.5
sd_ <- 5
x0_ <- 1000
t_ <- 100
pop_df <- multi_sim(nsim = numsim, x0 = x0_, t = t_, trend = trend_, sd = sd_)
#pop_df <- pop_df %>% mutate(diff_1 = pop - lag(pop))

# pop_df %>% mutate(run_mean = cumSkipNA(diff_1, sum)/(time - 1), # running mean
#                   run_var_mle = 0) %>%
  #rowwise() %>%
  # mutate(run_var_mle = ifelse(is.na(lag(run_mean) ) == TRUE, 0,
  #                         lag(run_var_mle) + (diff_1 + lag(run_mean)) * (diff_1 + run_mean) ),
  #        run_sd_mle = sqrt(run_var_mle / (time - 1)))

# for (i in seq_along(pop_df)) {
#   if (is.na(run_mean[i]) )
# }

# zoo::rollapply(pop_df$pop, width = 1, FUN = mean) %>% head()
# 
# stat_df <- pop_df$diff_1 %>% tail(-1) %>% run_stat()
# 
# left_join(pop_df,stat_df)

```

# Trend detection: sequential vs fixed sample approaches
Managers need to detect population trends in a timely and reliable manner. Timeliness is reflected in the number of samples (e.g., years) needed to detect a trend. Reliability is captured by the type I error and power of a test.

Linear regression is commonly used to detect trends in population data. For instance,
$$ N_{t+1} \sim N(N_t + r,\sigma), $$
Where $N_t$ is the population size at time $t$, $r$ is the population trend, and $\sigma$ is the population variability.

To identify the timeliness and reliability of linear regression for detecting population trends, [White (2019)](https://academic.oup.com/bioscience/article/69/1/40/5195956#129750432) repeatedly fit linear models to sub-samples of a time series to identify the minimum sample size needed to achieve a specified type I error and power. That is, for a time series with $T$ time points, he fit a linear regression model to each contiguous sub-samples of size $2, 3, \ldots, T-1$. Using the arbitrary (but conventional) benchmark of 0.8 power at the 0.05 significance level, White used the regression outputs to find the minimum sub-sample length such that 80 % of the samples had a significant regression slope coefficient. This is essentially a sample size calculation for a one-sided hypothesis test $H_0: r = 0$ vs $H_1: r > 0$.

We can also test this one-sided hypothesis using a sequential test. We can define our sequential test statistic $(S)$ as:

\begin{align*}
S_0 & = 0 \\
S_n & = \frac{P(N_{t+1} | N_t, \hat{r}, \sigma)}{P(N_{t+1} | N_t, r = 0, \sigma)}
\end{align*}

where $\hat{r}$ is the MLE of r constrained to  $r \in (0,\inf)$ or $r \in (-\inf, 0)$ depending on whether detection of a linear increase or decrease is desired. (In my computer implementation, I just use a finite interval for the constrained MLE).

The decision rule is:

\begin{align*}
    \begin{cases}
      \text{reject } H_0  & \Sigma^{n}_{r = 0} S_r > \log(A) \\
      \text{accept } H_0 & \Sigma^{n}_{r = 0} S_r \leq \log(B) \\
      \text{continue sampling} & \log(B)  <  \Sigma^{n}_{r = 0} S_r  \leq  \log(A)  
    \end{cases}
\end{align*}

The thresholds are set following Wald's method such that $A \sim \frac{1-\beta}{\alpha}$ and $B \sim \frac{\beta}{1-\alpha}$ where $\alpha$ and $\beta$ is the probabilities of type I and II error, respectively.

For the situation where $\sigma$ is unknown, the sequential test statistic is calculated using the current ML estimate of of the standard deviation ($\hat{\sigma}$), i.e.,

\begin{align*}
S_0 & = 0 \\
S_n & = \frac{P(N_{t+1} | N_t, \hat{r}, \hat{\sigma})}{P(N_{t+1} | N_t, r = 0, \hat{\sigma})}
\end{align*}

$\hat{\sigma}$ is computed as a [running variance](https://www.johndcook.com/blog/standard_deviation/) from the first differences of the observations $Z_t = X_t - X_{t-1} \overset{iid}{\sim} N(r,\sigma)$.

# Comparison of detection times

Tests:

1. fixed-sample size linear regression test
1. sequential test ($\sigma$ known)
1. sequential test ($\sigma$ unknown)
    + the running MLE estimate of $\sigma$ is used to calculate the test statistic
    + note that the MLE estimate of $\sigma$ is uncorrected (biased low)

I compare the performance of the tests according to the time required to reject the null hypothesis that there is no population trend ($\alpha = 0.05, \beta=0.2$). The population data were simulated from an additive model $X_{t+1} = X_t + r + N(0,\sigma)$ with parameters $r =$ `r trend_` and $\sigma =$ `r sd_`. The population was initialized as `r x0_` individuals and was run for `r t_` time steps. `r numsim` simulations were used.

```{r}
pop_df %>%
  ggplot() + geom_line(aes(x = time, y = pop, group = simulation), alpha = 0.2) +
  labs(title = "simulated population times series")
```

```{r cache = T, dependson="simulation"}
sprt_est <- 
  pop_df %>% #group_by(simulation) %>% 
  calc_sprt(sd_est = "sd_mle") %>%
  rename(decision_est = decision,
         sprt_est = sprt) %>%
  select(-lik0, -lik1)

sprt_known <- 
  pop_df %>% #group_by(simulation) %>% 
  calc_sprt(sd_est = "sd") %>%
  rename(decision_known = decision,
         sprt_known = sprt) %>%
  select(-lik0, -lik1)

sprt_df <- 
  full_join(sprt_est, sprt_known) 

delay_df <-
  full_join(sprt_df %>% detection_time_est(),
          sprt_df %>% detection_time_known()) %>%
  pivot_longer(cols = -simulation, names_to = "method", values_to = "delay") %>%
  group_by(method) %>%
  mutate(mean_delay = mean(delay, na.rm = TRUE))
```

```{r cache = T,dependson="simulation"}
samplesize_df <- 
  pop_df %>% seq_lm(nsim = numsim) %>%
  get_conclusion(sig_level = 0.05) %>%
  get_power(power_level = 0.8, nsim = numsim)
```

The fixed-sample size needs `r unique(samplesize_df$mintime)` samples to achieve 0.8 power at the 0.05 significance level. In comparison, the sequential tests require `r delay_df %>% filter(method == "delay_known") %>% pull(mean_delay) %>% unique() %>% ceiling()` ($\sigma$ known) and `r delay_df %>% filter(method == "delay_est") %>% pull(mean_delay) %>% unique() %>% ceiling()` ($\sigma$ unknown). The reason that the sequential test where $\sigma$ is unknown detects the change faster than when $\sigma$ is known (for the parameters used here) is because the MLE estimate of $\sigma$ is uncorrected which biases it low. This makes the likelihood of the null smaller than it should be. The table belows shows the number of instances either sequential method required as many or more samples than the fixed-sample size test.

```{r}
samplesize <- unique(samplesize_df$mintime)
delay_df %>% count(delay >= samplesize) %>% knitr::kable()

```

```{r warning=FALSE}
delay_df %>% 
  ggplot() +
  geom_histogram(aes(x = delay, fill = method), binwidth = 1) +
  geom_vline(aes(xintercept = mean_delay, linetype = method), size = 1.5) +
  geom_vline(aes(xintercept = mean(samplesize_df$mintime)), col = "red", size = 1.5) +
  xlim(0, 30) +
  labs(title = "Detection times for population trend",
       subtitle = "red line is minimum time for fixed-sample method")
```

Note, the probability models used for the fixed-sample and sequential tests are not the same. The sequential test uses the true data-generating model (variability is from process noise) whereas the fixed-sample test fits a simple linear regression using time as a covariate (observation noise). A more comparable test would be to use a sequential t-test on the first-differenced values of the population sizes. 

# Performance when null is true (no trend)

```{r simulation_null, cache=T}
knitr::read_chunk("trend_detection_functions.R")
set.seed(123)
numsim <- 100
# trend_ <- 1.5
sd_ <- 5
x0_ <- 1000
t_ <- 100
pop_df_null <- multi_sim(nsim = numsim, x0 = x0_, t = t_, trend = 0, sd = sd_)
```

How do the tests perform when there is no trend? The sequential tests would (ideally) accept the null hypothesis. The fixed-sample procedure cannot accept the null, so I will consider any non-significant p-values to indicate that the null was "accepted."

```{r}
pop_df_null %>%
  ggplot() + geom_line(aes(x = time, y = pop, group = simulation), alpha = 0.2) +
  labs(title = "simulated population times series")
```

I don't like the behavior of this model when there is no trend. The distribution isn't stationary (constant mean and variance over time) so I don't think this is a fair test.

```{r cache = T, dependson="simulation_null"}
sprt_est_null <- 
  pop_df_null %>% #group_by(simulation) %>% 
  calc_sprt(sd_est = "sd_mle") %>%
  rename(decision_est = decision,
         sprt_est = sprt) %>%
  select(-lik0, -lik1)

sprt_known_null <- 
  pop_df_null %>% #group_by(simulation) %>% 
  calc_sprt(sd_est = "sd") %>%
  rename(decision_known = decision,
         sprt_known = sprt) %>%
  select(-lik0, -lik1)

sprt_df_null <- 
  full_join(sprt_est_null, sprt_known_null) 

delay_df_null <-
  full_join(
  sprt_df_null %>%
  group_by(time) %>%
  count(decision_est, name = "n_est"),
sprt_df_null %>%
  group_by(time) %>%
  count(decision_known, name = "n_known")
)
```


```{r cache = T,dependson="simulation_null"}
samplesize_df_null <- 
  pop_df_null %>% seq_lm(nsim = numsim) %>%
  get_conclusion(sig_level = 0.05) #%>%
  #get_power(power_level = 0.8, nsim = numsim)

mintime_df_null <-
  samplesize_df_null %>% 
    group_by(time) %>%
    count(type_error) %>%
    filter(type_error %in% c("significant_true","significant_false") )
```

```{r fpr_plot}
delay_df_null %>%
  ggplot() +
  geom_line(aes(x = time, y = n_est, col = decision_est), size = 1.5, linetype = "solid") +
  geom_line(aes(x = time, y = n_known, col = decision_known), size = 1.5, linetype = "dashed") +
  geom_line(data = mintime_df_null, aes(x = time, y = n), size = 1.5) +
  labs(title = "False positive rates of tests",
       subtitle = "Solid = sd unknown; Dashed = sd known; black = fixed-sample test",
       y = "frequency of false positives")
```

All of the tests perform quite poorly. But, I think that this is more an issue with the null model. It would be better to simulate from a gompertz model at (stochastic) equilibrium to generate test data.

```{r eval = FALSE}
# plot FPR individually

delay_df_null %>%
  ggplot() +
  geom_line(aes(x = time, y = n_est, col = decision_est), size = 1.5, linetype = "solid") +
  geom_line(aes(x = time, y = n_known, col = decision_known), size = 1.5, linetype = "dashed") +
  geom_line(data = mintime_df_null, aes(x = time, y = n), size = 1.5, col = "red") +
  labs(title = "False positive rate of sequential test",
       subtitle = "Solid = sd unknown; Dashed = sd known")

mintime_df_null %>%  
ggplot() +
  geom_line(aes(x = time, y = n)) +
  labs(title = "False positive rate for fixed-sample test") +
  ylim(0,100)
 
```

# Numerical problems
Below are tables showing the first times a simulation yields a test statistic that is +/- Inf. This (so far) only happens when using estimates of sd. As you can see, in these cases the estimates of sd are all very small. This results in likelihoods that are 0 which will mess up the calculation of the log-likelihood ratio. This is an inferential problem, because if the test statistic is ever +/- Inf, then no amount of new evidence can ever change the conclusion. So these early underestimates of sd are a problem for the sd unknown method.

```{r}
sprt_df_null %>% filter(sprt_est %in% c(-Inf,Inf) | sprt_known %in% c(-Inf,Inf)) %>% 
  group_by(simulation) %>% filter(time == first(time)) %>%
  knitr::kable(caption = "Infinite sprt when null is true")

sprt_df %>% filter(sprt_est %in% c(-Inf,Inf) | sprt_known %in% c(-Inf,Inf)) %>% 
  group_by(simulation) %>% filter(time == first(time)) %>%
  knitr::kable(caption = "Infinite sprt when null is false")
```

Tried using calculating sprt as a difference of logs instead (below). Still have issue with +/- Inf values. Note, this isn't an issue for the sprt calculation when sd is known. I've noticed that all instances where the sprt %in% c(-Inf,Inf) that it is because the early estimate of sd is very small. Maybe, need to include a way to wait until estimate of sd stabilizes. 

```{r}
sprt_est_alt <- 
  pop_df %>% #group_by(simulation) %>% 
  calc_sprt_alt(sd_est = "sd_mle") %>%
  rename(decision_est = decision,
         sprt_est = sprt) #%>%
  #select(-lik0, -lik1)

sprt_est_alt %>% filter(sprt_est %in% c(-Inf,Inf)) %>% 
  group_by(simulation) %>% filter(time == first(time)) %>%
  knitr::kable(caption = "Infinite sprt when null is false")
```

So how should I handle cases where the likelihood when one (or both) of the hypotheses is zero? It's hard to come up with a rule for handling this, because the absolute magnitude of likelihoods is meaningless - it is the relative magnitude of likelihoods that matters. Both likelihoods tend to be very small in these cases, but it is still necessary to compare them which is challenging since they are so small.

# to-dos

* address cases where p(data|hypothesis) = 0.
* test performance on time-series with no trend (should be stationary dist'n; include burn-in time)
* simulation studies using different trends and sd

